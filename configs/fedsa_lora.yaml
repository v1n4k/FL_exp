method: fedsa_lora
seed: 42

lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules: [query, value]
  base_model: roberta-large

rpca:
  max_iter: 50
  tol: 1.0e-6
  lam: null

data:
  dataset_name: glue
  glue_task: sst2
  max_length: 128
  dirichlet_alpha: 0.5

train:
  num_clients: 5
  num_rounds: 3
  local_epochs: 1
  batch_size: 8
  lr: 3.0e-4
  device: null
  init_noise_std: 0.0
  gpus_per_client: 0.5
  optimizer: sgd
  momentum: 0.9
  weight_decay: 0.0
  early_stop_patience: 3
  orthogonal_reg_weight: 0.0
  orthogonal_reg_warmup_steps: 0
  grad_clip_norm: 0.0
  client_cache_dir: client_cache_lora

extra:
  use_wandb: "True"
  wandb_project: fedsa-lora-baseline
  wandb_job_type: fedsa_lora
  wandb_group: baseline_001
